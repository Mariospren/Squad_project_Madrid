{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Merge the twelve dataframes, spanning from January to December for both specified years, into a single dataframe. Then, display the total dimensions (shape) of the resulting combined dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mario\\AppData\\Local\\Temp\\ipykernel_17244\\2004468919.py:23: DtypeWarning: Columns (10,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_name, delimiter=';', encoding='latin-1')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "def combine_csv_files(folder_path):\n",
    "    \"\"\"\n",
    "    Combine multiple CSV files in a given folder into a single DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path (str): The path to the folder containing CSV files.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Combined DataFrame.\n",
    "    \"\"\"\n",
    "    # Get the list of CSV file names in the folder\n",
    "    file_pattern = '*.csv'\n",
    "    file_names = glob.glob(folder_path + file_pattern)\n",
    "\n",
    "    # List to store DataFrames\n",
    "    df_list = []\n",
    "\n",
    "    # Read each CSV file and append its DataFrame to the list\n",
    "    for file_name in file_names:\n",
    "        df = pd.read_csv(file_name, delimiter=';', encoding='latin-1')\n",
    "        df_list.append(df)\n",
    "\n",
    "    # Concatenate the DataFrames into a single DataFrame\n",
    "    df_combined = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    return df_combined\n",
    "\n",
    "Folder_2020 =  './Fines2020/'\n",
    "Folder_2022 =  './Fines2022/'\n",
    "\n",
    "df_2020 = combine_csv_files(Folder_2020)\n",
    "df_2022 = combine_csv_files(Folder_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1993304, 18)\n",
      "(2702125, 14)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['CALIFICACION', 'LUGAR', 'MES', 'ANIO', 'HORA', 'IMP_BOL', 'DESCUENTO',\n",
       "       'PUNTOS', 'DENUNCIANTE', 'HECHO-BOL', 'VEL_LIMITE', 'VEL_CIRCULA',\n",
       "       'COORDENADA_X', 'COORDENADA_Y', ' PUNTOS', 'VEL_CIRCULA ',\n",
       "       'COORDENADA-X',\n",
       "       'COORDENADA-Y                                                                                                                                '],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_2020.shape)\n",
    "print(df_2022.shape)\n",
    "df_2020.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. As we will not be working with geographic data, delete the COORDENADA-X and COORDENADA-Y columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['CALIFICACION', 'LUGAR', 'MES', 'ANIO', 'HORA', 'IMP_BOL', 'DESCUENTO',\n",
      "       'PUNTOS', 'DENUNCIANTE', 'HECHO-BOL', 'VEL_LIMITE', 'VEL_CIRCULA'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def drop_coors(df):\n",
    "    \"\"\"\n",
    "    Clean a DataFrame by fixing column names, removing duplicates, and dropping specified columns.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Cleaned DataFrame with fixed column names, removed duplicates,\n",
    "                  and specified columns dropped.\n",
    "    \"\"\"\n",
    "    # Fix column names\n",
    "    df.columns = df.columns.str.strip().str.replace(' ', '_').str.upper()\n",
    "\n",
    "    # Remove duplicate columns\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "    # Drop unwanted columns without modifying names\n",
    "    columns_to_drop = ['COORDENADA_X', 'COORDENADA_Y', 'COORDENADA-X', 'COORDENADA-Y']\n",
    "    df_cleaned = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "df_2020 = drop_coors(df_2020)\n",
    "df_2022 = drop_coors(df_2022)\n",
    "\n",
    "# Print the columns of the resulting DataFrame\n",
    "print(df_2020.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Examine the unique values of the variables. Some may have unique values, making them constants. Evaluate their relevance in the dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANIO has only one unique value.\n",
      "DESCUENTO has only one unique value.\n",
      "ANIO has only one unique value.\n"
     ]
    }
   ],
   "source": [
    "def nunique_value(df):\n",
    "    for column in df.columns:\n",
    "        unique_values = df[column].nunique()\n",
    "        if unique_values == 1:\n",
    "         a = print(f\"{column} has only one unique value.\")\n",
    "    \n",
    "    return a\n",
    "\n",
    "        \n",
    "\n",
    "nun_2020 = nunique_value(df_2020)\n",
    "nun_2022 = nunique_value(df_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DESCUENTO\n",
       "SI    2702124\n",
       "NO          1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2022['DESCUENTO'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CALIFICACION</th>\n",
       "      <th>LUGAR</th>\n",
       "      <th>MES</th>\n",
       "      <th>ANIO</th>\n",
       "      <th>HORA</th>\n",
       "      <th>IMP_BOL</th>\n",
       "      <th>DESCUENTO</th>\n",
       "      <th>PUNTOS</th>\n",
       "      <th>DENUNCIANTE</th>\n",
       "      <th>HECHO-BOL</th>\n",
       "      <th>VEL_LIMITE</th>\n",
       "      <th>VEL_CIRCULA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>358869</th>\n",
       "      <td>LEVE</td>\n",
       "      <td>PUENTE VALLECAS CALZADA 1</td>\n",
       "      <td>2</td>\n",
       "      <td>2022</td>\n",
       "      <td>13.45</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>POLICIA MUNICIPAL</td>\n",
       "      <td>CIRCULAR TRANSPORTANDO MERCANCÍAS PELIGROSAS S...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CALIFICACION                                     LUGAR  MES  ANIO  \\\n",
       "358869   LEVE        PUENTE VALLECAS CALZADA 1                   2  2022   \n",
       "\n",
       "         HORA  IMP_BOL DESCUENTO  PUNTOS           DENUNCIANTE  \\\n",
       "358869  13.45     30.0        NO       0  POLICIA MUNICIPAL      \n",
       "\n",
       "                                                HECHO-BOL VEL_LIMITE  \\\n",
       "358869  CIRCULAR TRANSPORTANDO MERCANCÍAS PELIGROSAS S...              \n",
       "\n",
       "       VEL_CIRCULA  \n",
       "358869              "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_2022[df_2022['DESCUENTO'] == 'NO']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"We found that ANIO (year) and DESCUENTO (discount) are always constant. I'm not sure to what extent they are relevant in our DataFrame, as we know that our DataFrame already has the year classified, making the ANIO column redundant. Additionally, the DESCUENTO column seems not to vary, except for an outlier we found in the 2022 DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. The VEL_LIMITE and VEL_CIRCULA columns appear to have empty values. This occurs when the infraction is not related to a speed limit. Convert all anomalies to null values. Hint: Explore the use of regex (regular expressions). Investigate the use of a pattern like r'^\\s*$' within the replace function to handle specific data anomalies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. The columns VEL_LIMITE and VEL_CIRCULA, though numerical, are recognized as \"object\" type. After checking the frequency of values, you'll find they are counted as text strings. Therefore:\n",
    "\n",
    " a. Convert nulls in these two variables to 0.\n",
    "\n",
    "  b. Change the variable type to numeric.\n",
    "\n",
    "c. Identify the most common speed limit, excluding zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Create a new column called DIFFERENCE_KMH, calculated by subtracting the speed limit from the driver's speed at the time of the infraction. Use this to identify the top 10 drivers who exceeded the speed limits by the highest margins.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Filter the dataframe for complaints that resulted in point deductions (other than zero points). Group this data by the public agent issuing the complaint. Which agent has the highest average points deduction?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Remove the decimal part of the hours column and graphically represent the number of infractions for each hour. Identify the peak hours with the most infractions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Graphically display the fines issued during the months of the most recent full year available. Can any analysis be derived from this data? Compare it with the 2020 analysis to potentially predict the impact of Spain's COVID-19 confinement on traffic infractions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Display the number of infractions (without accumulated frequency) by each public agent, categorized by the infraction classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Imagine landing a Junior Data Scientist role at a firm specializing in the comparative analysis of fines. Your boss, setting high standards, assigns you to conduct a comparative study between data from the most recent full year and the available 2020 records. You have the discretion to include additional data from other years if it enhances the analysis. Use relevant graphics to augment your analysis and weave an engaging narrative!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
